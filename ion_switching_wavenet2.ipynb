{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ion-switching-wavenet2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "a40NOJnmC3Nj"
      ],
      "authorship_tag": "ABX9TyPgGqfgFzBbffI/bidV03T9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsyvy/ion-switching/blob/master/ion_switching_wavenet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a40NOJnmC3Nj",
        "colab_type": "text"
      },
      "source": [
        "# Colab environment development\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGP4Lr3BCf_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqQkgpjuC-QA",
        "colab_type": "text"
      },
      "source": [
        "#imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRlxo0DTCm6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9e3e7bdc-b7a9-4145-a8ae-10e873699107"
      },
      "source": [
        "# imports\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "from typing import (List, NoReturn, Union, Tuple, Optional, \n",
        "                    Text, Generic, Callable, Dict)\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.model_selection import KFold, GroupKFold, GroupShuffleSplit, train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import models, losses\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence, to_categorical, get_custom_objects\n",
        "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.layers import  Conv1D, Activation, Input, Dense, Add, Multiply\n",
        "\n",
        "\n",
        "\n",
        "from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMuv9VlwDXBU",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9V0jsLDA8J",
        "colab_type": "code",
        "outputId": "9c0fe964-df2c-4145-fa08-4006fcae2e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "# clean data without feature engineering\n",
        "train = pd.read_csv(\"gdrive/My Drive/data/ion-switching/train_clean.csv\")\n",
        "# test = pd.read_csv(\"gdrive/My Drive/data/ion-switching/test_clean.csv\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 885 ms, sys: 156 ms, total: 1.04 s\n",
            "Wall time: 1.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V84CcWDtDZ9u",
        "colab_type": "code",
        "outputId": "3dccbe8d-0809-473f-dd91-203f8db3d82f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>signal</th>\n",
              "      <th>open_channels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>-2.7600</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0002</td>\n",
              "      <td>-2.8557</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0003</td>\n",
              "      <td>-2.4074</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0004</td>\n",
              "      <td>-3.1404</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0005</td>\n",
              "      <td>-3.1525</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     time  signal  open_channels\n",
              "0  0.0001 -2.7600              0\n",
              "1  0.0002 -2.8557              0\n",
              "2  0.0003 -2.4074              0\n",
              "3  0.0004 -3.1404              0\n",
              "4  0.0005 -3.1525              0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFevDmRtDq2n",
        "colab_type": "text"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp2TXLKpYicc",
        "colab_type": "text"
      },
      "source": [
        "## log manager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY7uoUIQR22-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_logger():\n",
        "    handler = StreamHandler()\n",
        "    handler.setLevel(INFO)\n",
        "    handler.setFormatter(Formatter(LOGFORMAT))\n",
        "    fh_handler = FileHandler('{}.log'.format(MODELNAME))\n",
        "    fh_handler.setFormatter(Formatter(LOGFORMAT))\n",
        "    logger.setLevel(INFO)\n",
        "    logger.addHandler(handler)\n",
        "    logger.addHandler(fh_handler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9e0gxwVR63L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@contextmanager\n",
        "def timer(name : Text):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
        "\n",
        "COMPETITION = 'ION-Switching'\n",
        "logger = getLogger(COMPETITION)\n",
        "LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
        "MODELNAME = 'WaveNet'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYGheb9YoK8",
        "colab_type": "text"
      },
      "source": [
        "## Seed everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FV3ZLObSAF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed : int) -> NoReturn :\n",
        "        \n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "seed_everything(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3DKLmzpYtW6",
        "colab_type": "text"
      },
      "source": [
        "## Data helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQeJqEBHSH-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \n",
        "    train = pd.read_csv('gdrive/My Drive/data/ion-switching/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
        "    test  = pd.read_csv('gdrive/My Drive/data/ion-switching/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
        "    \n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZp_6ItJErys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batching(df : pd.DataFrame,\n",
        "             batch_size : int) -> pd.DataFrame :\n",
        "    \n",
        "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
        "    df['group'] = df['group'].astype(np.uint16)\n",
        "        \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FekamMHTE-NC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lag_with_pct_change(df : pd.DataFrame,\n",
        "                        shift_sizes : Optional[List]=[1, 2],\n",
        "                        add_pct_change : Optional[bool]=False,\n",
        "                        add_pct_change_lag : Optional[bool]=False) -> pd.DataFrame:\n",
        "    \n",
        "    for shift_size in shift_sizes:    \n",
        "        df['signal_shift_pos_'+str(shift_size)] = df.groupby('group')['signal'].shift(shift_size).fillna(method='bfill')\n",
        "        df['signal_shift_neg_'+str(shift_size)] = df.groupby('group')['signal'].shift(-1*shift_size).fillna(method='ffill')\n",
        "\n",
        "    if add_pct_change:\n",
        "        df['pct_change'] = df['signal'].pct_change()\n",
        "        if add_pct_change_lag:\n",
        "            for shift_size in shift_sizes:    \n",
        "                df['pct_change_shift_pos_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(shift_size).fillna(method='bfill')\n",
        "                df['pct_change_shift_neg_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(-1*shift_size).fillna(method='ffill')\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGXzT1ZVG13p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_feat_enginnering(df : pd.DataFrame,\n",
        "                         create_all_data_feats : bool,\n",
        "                         batch_size : int) -> pd.DataFrame:\n",
        "    \n",
        "    df = batching(df, batch_size=batch_size)\n",
        "    if create_all_data_feats:\n",
        "        df = lag_with_pct_change(df, [1, 2, 3],  add_pct_change=False, add_pct_change_lag=False)\n",
        "    df['signal_2'] = df['signal'] ** 2\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ENtA35HBEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_selection(df : pd.DataFrame) -> Tuple[pd.DataFrame, List]:\n",
        "    use_cols = [col for col in df.columns if col not in ['index','group', 'open_channels', 'time']]\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    for col in use_cols:\n",
        "        col_mean = df[col].mean()\n",
        "        df[col] = df[col].fillna(col_mean)\n",
        "\n",
        "    gc.collect()\n",
        "    return df, use_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQhSGiasHOhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(X: np.array, y:np.array) -> Tuple[np.array, np.array]:\n",
        "    \n",
        "    X = np.vstack((X, np.flip(X, axis=1)))\n",
        "    y = np.vstack((y, np.flip(y, axis=1)))\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CIxG4wVPFgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(train, test):\n",
        "    \n",
        "    train_input_mean = train.signal.mean()\n",
        "    train_input_sigma = train.signal.std()\n",
        "    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n",
        "    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n",
        "\n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCHZAs7TYylr",
        "colab_type": "text"
      },
      "source": [
        "## Lr scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMJEyAphIycp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    if epoch < 10:\n",
        "        lr = LR\n",
        "    elif epoch < 15:\n",
        "        lr = LR / 3\n",
        "    elif epoch < 20:\n",
        "        lr = LR / 6\n",
        "    elif epoch < 75:\n",
        "        lr = LR / 9\n",
        "    elif epoch < 85:\n",
        "        lr = LR / 12\n",
        "    elif epoch < 100:\n",
        "        lr = LR / 15\n",
        "    else:\n",
        "        lr = LR / 50\n",
        "    return lr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W-tfw2vY3jo",
        "colab_type": "text"
      },
      "source": [
        "## Mish activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_MbSZF0I3W9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mish(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Mish, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs * K.tanh(K.softplus(inputs))\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(Mish, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "def mish(x):\n",
        "\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n",
        " \n",
        "\n",
        "get_custom_objects().update({'mish': Activation(mish)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOsCyQILY7gf",
        "colab_type": "text"
      },
      "source": [
        "## Focal loss and F1 matric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr4bK-RkM207",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_focal_loss(gamma=2.0, alpha=0.25):\n",
        "    \"\"\"\n",
        "    Implementation of Focal Loss from the paper in multiclass classification\n",
        "    Formula:\n",
        "        loss = -alpha*((1-p)^gamma)*log(p)\n",
        "    Parameters:\n",
        "        alpha -- the same as wighting factor in balanced cross entropy\n",
        "        gamma -- focusing parameter for modulating factor (1-p)\n",
        "    Default value:\n",
        "        gamma -- 2.0 as mentioned in the paper\n",
        "        alpha -- 0.25 as mentioned in the paper\n",
        "    \"\"\"\n",
        "    def focal_loss(y_true, y_pred):\n",
        "        epsilon = K.epsilon()\n",
        "        \n",
        "        # Clip the prediction value\n",
        "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
        "        # Calculate cross entropy\n",
        "        cross_entropy = -y_true*K.log(y_pred)\n",
        "        # Calculate weight that consists of  modulating factor and weighting factor\n",
        "        weight = alpha * y_true * K.pow((1-y_pred), gamma)\n",
        "        # Calculate focal loss\n",
        "        loss = weight * cross_entropy\n",
        "        # Sum the losses in mini_batch\n",
        "        loss = K.sum(loss, axis=1)\n",
        "        return loss\n",
        "    \n",
        "    return focal_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnEjiIPJO80t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MacroF1(Callback):\n",
        "    def __init__(self, model, inputs, targets):\n",
        "        self.model = model\n",
        "        self.inputs = inputs\n",
        "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
        "        score = f1_score(self.targets, pred, average=\"macro\")\n",
        "        print(f' F1Macro: {score:.5f}')    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyzS9fo5Wn0n",
        "colab_type": "text"
      },
      "source": [
        "# parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk9jcrGWWrBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=9\n",
        "NNBATCHSIZE=20\n",
        "BATCHSIZE = 10000\n",
        "SEED = 23\n",
        "SELECT = True\n",
        "LR = 0.001\n",
        "fe_config = [\n",
        "    (True, 10000),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FRvk6wcRANL",
        "colab_type": "text"
      },
      "source": [
        "# wavenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLvgYWtMNpIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n",
        "    def build_residual_block(l_input):\n",
        "        resid_input = l_input\n",
        "        for dilation_rate in [2**i for i in range(stacked_layer)]:\n",
        "            l_sigmoid_conv1d = Conv1D(num_filters, kernel_size, \n",
        "                                      dilation_rate=dilation_rate,\n",
        "                                      padding='same', \n",
        "                                      activation='sigmoid')(l_input)\n",
        "            l_tanh_conv1d = Conv1D(num_filters, kernel_size, \n",
        "                                   dilation_rate=dilation_rate,\n",
        "                                   padding='same', \n",
        "                                   activation='mish')(l_input)\n",
        "            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n",
        "            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n",
        "            resid_input = Add()([resid_input ,l_input])\n",
        "        return resid_input\n",
        "    return build_residual_block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dw4VSePOn5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Classifier(shape_):\n",
        "    num_filters_ = 16\n",
        "    kernel_size_ = 3    \n",
        "    stacked_layers_ = [12, 8, 4, 1]\n",
        "    inp = Input(shape=(shape_))\n",
        "    x = Conv1D(num_filters_, 1, padding='same')(inp)\n",
        "    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n",
        "    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n",
        "    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n",
        "    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n",
        "\n",
        "    out = Dense(11, activation='softmax')(x)\n",
        "    model = models.Model(inputs=[inp], outputs=[out])\n",
        "    return model\n",
        "\n",
        "# opt = Adam(lr=LR)\n",
        "# opt = tfa.optimizers.SWA(opt)\n",
        "# model = Classifier()\n",
        "# model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMkl1owgb4gN",
        "colab_type": "text"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeTEURpQdF05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_train(train : pd.DataFrame,\n",
        "                batch_col : Text,\n",
        "                feats : List,\n",
        "                nn_epochs : int,\n",
        "                nn_batch_size : int) -> NoReturn:\n",
        "    \n",
        "    seed_everything(SEED)\n",
        "\n",
        "    # tensorflow session \n",
        "    K.clear_session()\n",
        "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "    \n",
        "\n",
        "    # train and val data split\n",
        "    group = train['group']\n",
        "    train_id = list()\n",
        "    val_id = list()\n",
        "    for i in range(0, 5000000, 500000):\n",
        "        train_id.extend(list(range(i, i+450000)))\n",
        "        val_id.extend(list(range(i+450000, i+500000)))\n",
        "    train_id = np.asarray(train_id)\n",
        "    val_id = np.asarray(val_id)\n",
        "    train_val_ids = []\n",
        "    train_val_ids.append(np.unique(group[train_id]))\n",
        "    train_val_ids.append(np.unique(group[val_id]))\n",
        " \n",
        "\n",
        "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
        "\n",
        "    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
        "    target_cols = ['target_'+str(i) for i in range(11)]\n",
        "    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
        "    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
        "\n",
        "\n",
        "    train_x, train_y = train[train_val_ids[0]], train_tr[train_val_ids[0]]\n",
        "    valid_x, valid_y = train[train_val_ids[1]], train_tr[train_val_ids[1]]\n",
        "        \n",
        "    train_x, train_y = augment(train_x, train_y)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    shape_ = (None, train_x.shape[2])\n",
        "    model = Classifier(shape_)\n",
        "    opt = Adam(lr=LR)\n",
        "    opt = tfa.optimizers.SWA(opt)\n",
        "    model.compile(loss=categorical_focal_loss(), optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "    cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
        "    cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,\n",
        "                                           leave_overall_progress=False, \n",
        "                                           show_epoch_progress=True,\n",
        "                                           show_overall_progress=True)       \n",
        "    model.fit(train_x,train_y,\n",
        "              epochs=nn_epochs,\n",
        "            #   callbacks=[cb_lr_schedule],\n",
        "              batch_size=nn_batch_size, verbose=1,\n",
        "              validation_data=(valid_x,valid_y))\n",
        "    \n",
        "    # predictions and performance matrics\n",
        "    y_preds = model.predict(valid_x)\n",
        "    y_true_ = np.argmax(valid_y, axis=2).reshape(-1)\n",
        "    y_preds_ = np.argmax(y_preds, axis=2).reshape(-1)\n",
        "    accuracy_score_ = accuracy_score(y_true_, y_preds_)\n",
        "    precision_score_ = precision_score(y_true_, y_preds_, average = 'macro')\n",
        "    recall_score_ = recall_score(y_true_, y_preds_, average = 'macro')\n",
        "    f1_score_ = f1_score(y_true_, y_preds_, average = 'macro')\n",
        "    cm = confusion_matrix(y_true_, y_preds_, labels=np.unique(y_true_))\n",
        "\n",
        "    \n",
        "    logger.info(f'Training completed')\n",
        "    logger.info(f'accuracy score: {accuracy_score_:1.5f}')\n",
        "    logger.info(f'precision score: {precision_score_:1.5f}')\n",
        "    logger.info(f'recall score: {recall_score_:1.5f}')\n",
        "    logger.info(f'macro f1 score: {f1_score_:1.5f}')\n",
        "    logger.info(f'confusion matrix\\n{cm}')\n",
        "\n",
        "    return \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwo3TYGalZXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_all():\n",
        "    not_feats_cols = ['time']\n",
        "    target_col = ['open_channels']\n",
        "    init_logger()\n",
        "    with timer(f'Reading Data'):\n",
        "        logger.info('Reading Data Started ...')\n",
        "        train, test = read_data()\n",
        "        train, test = normalize(train, test)    \n",
        "        logger.info('Reading and Normalizing Data Completed ...')\n",
        "    with timer(f'Creating Features'):\n",
        "        logger.info('Feature Enginnering Started ...')\n",
        "        for config in fe_config:\n",
        "            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n",
        "        train, feats = feature_selection(train)\n",
        "        logger.info('Feature Enginnering Completed ...')\n",
        "\n",
        "    with timer(f'Running Wavenet model'):\n",
        "        model_train(train, batch_col='group', feats=feats,  nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n",
        "        logger.info(f'Training completed ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNabwWweNuLw",
        "colab_type": "code",
        "outputId": "0a58d38e-43b9-4066-97db-d1d49f731aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "run_all()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-03 17:29:09,332 INFO Reading Data Started ...\n",
            "2020-06-03 17:29:10,775 INFO Reading and Normalizing Data Completed ...\n",
            "2020-06-03 17:29:10,776 INFO [Reading Data] done in 1 s\n",
            "2020-06-03 17:29:10,778 INFO Feature Enginnering Started ...\n",
            "2020-06-03 17:29:13,690 INFO Feature Enginnering Completed ...\n",
            "2020-06-03 17:29:13,691 INFO [Creating Features] done in 3 s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "45/45 [==============================] - 16s 360ms/step - loss: 121.2756 - accuracy: 0.6884 - val_loss: 54.4157 - val_accuracy: 0.8288\n",
            "Epoch 2/9\n",
            "45/45 [==============================] - 13s 293ms/step - loss: 44.4338 - accuracy: 0.8666 - val_loss: 23.6422 - val_accuracy: 0.9151\n",
            "Epoch 3/9\n",
            "45/45 [==============================] - 13s 293ms/step - loss: 18.9142 - accuracy: 0.9330 - val_loss: 11.2660 - val_accuracy: 0.9481\n",
            "Epoch 4/9\n",
            "45/45 [==============================] - 13s 292ms/step - loss: 10.1861 - accuracy: 0.9550 - val_loss: 6.9352 - val_accuracy: 0.9636\n",
            "Epoch 5/9\n",
            "45/45 [==============================] - 13s 292ms/step - loss: 7.9574 - accuracy: 0.9612 - val_loss: 6.8401 - val_accuracy: 0.9637\n",
            "Epoch 6/9\n",
            "45/45 [==============================] - 13s 293ms/step - loss: 7.2419 - accuracy: 0.9629 - val_loss: 6.4256 - val_accuracy: 0.9643\n",
            "Epoch 7/9\n",
            "45/45 [==============================] - 13s 292ms/step - loss: 7.3751 - accuracy: 0.9621 - val_loss: 6.0553 - val_accuracy: 0.9661\n",
            "Epoch 8/9\n",
            "45/45 [==============================] - 13s 292ms/step - loss: 6.7684 - accuracy: 0.9643 - val_loss: 5.8179 - val_accuracy: 0.9672\n",
            "Epoch 9/9\n",
            "45/45 [==============================] - 13s 294ms/step - loss: 6.5601 - accuracy: 0.9646 - val_loss: 5.6661 - val_accuracy: 0.9676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-06-03 17:31:58,629 INFO Training completed\n",
            "2020-06-03 17:31:58,632 INFO accuracy score: 0.96763\n",
            "2020-06-03 17:31:58,633 INFO precision score: 0.93484\n",
            "2020-06-03 17:31:58,635 INFO recall score: 0.93676\n",
            "2020-06-03 17:31:58,636 INFO macro f1 score: 0.93578\n",
            "2020-06-03 17:31:58,638 INFO confusion matrix\n",
            "[[123785    226      0      0      0      0      0      0      0      0\n",
            "       0]\n",
            " [   315  98565    323      0      0      0      0      0      0      0\n",
            "       0]\n",
            " [     0    480  54906    561      0      0      1      0      0      0\n",
            "       0]\n",
            " [     0      3    864  65348    821      0      0      0      0      0\n",
            "       0]\n",
            " [     0      0      7    488  38939    799      0      0      0      0\n",
            "       0]\n",
            " [     0      0      1     28    622  25172    862      0      0      0\n",
            "       0]\n",
            " [     0      0      0      0      2    755  16581   1574      0      0\n",
            "       0]\n",
            " [     0      0      1      0      0      1   1231  23622   1590      0\n",
            "       0]\n",
            " [     0      0      0      0      0      0      0   1636  21700   1238\n",
            "       0]\n",
            " [     0      0      0      2      0      0      0      0    973  12126\n",
            "     426]\n",
            " [     0      0      0      0      0      0      0      0      0    357\n",
            "    3069]]\n",
            "2020-06-03 17:31:58,643 INFO Training completed ...\n",
            "2020-06-03 17:31:58,644 INFO [Running Wavenet model] done in 165 s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATMq5JlONuyl",
        "colab_type": "text"
      },
      "source": [
        "# logs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALREdDgOEEC8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caoWKrOCN893",
        "colab_type": "text"
      },
      "source": [
        "2020-06-01 12:33:42,685 INFO Training fold 1 completed. macro f1 score : 0.93829\n",
        "\n",
        "2020-06-01 13:19:43,528 INFO Training fold 2 completed. macro f1 score : 0.93830\n",
        "\n",
        "2020-06-01 13:44:03,020 INFO Training fold 3 completed. macro f1 score : 0.93727\n",
        "\n",
        "2020-06-01 14:08:18,136 INFO Training fold 4 completed. macro f1 score : 0.93784\n",
        "\n",
        "2020-06-01 14:32:34,309 INFO Training fold 5 completed. macro f1 score : 0.93721\n",
        "\n",
        "2020-06-01 14:32:38,380 INFO Training completed. oof macro f1 score : 0.93779\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVtjQOQGENHo",
        "colab_type": "text"
      },
      "source": [
        "45/45 [==============================] - 16s 360ms/step - loss: 121.2756 - accuracy: 0.6884 - val_loss: 54.4157 - val_accuracy: 0.8288\n",
        "Epoch 2/9\n",
        "45/45 [==============================] - 13s 293ms/step - loss: 44.4338 - accuracy: 0.8666 - val_loss: 23.6422 - val_accuracy: 0.9151\n",
        "Epoch 3/9\n",
        "45/45 [==============================] - 13s 293ms/step - loss: 18.9142 - accuracy: 0.9330 - val_loss: 11.2660 - val_accuracy: 0.9481\n",
        "Epoch 4/9\n",
        "45/45 [==============================] - 13s 292ms/step - loss: 10.1861 - accuracy: 0.9550 - val_loss: 6.9352 - val_accuracy: 0.9636\n",
        "Epoch 5/9\n",
        "45/45 [==============================] - 13s 292ms/step - loss: 7.9574 - accuracy: 0.9612 - val_loss: 6.8401 - val_accuracy: 0.9637\n",
        "Epoch 6/9\n",
        "45/45 [==============================] - 13s 293ms/step - loss: 7.2419 - accuracy: 0.9629 - val_loss: 6.4256 - val_accuracy: 0.9643\n",
        "Epoch 7/9\n",
        "45/45 [==============================] - 13s 292ms/step - loss: 7.3751 - accuracy: 0.9621 - val_loss: 6.0553 - val_accuracy: 0.9661\n",
        "Epoch 8/9\n",
        "45/45 [==============================] - 13s 292ms/step - loss: 6.7684 - accuracy: 0.9643 - val_loss: 5.8179 - val_accuracy: 0.9672\n",
        "Epoch 9/9\n",
        "45/45 [==============================] - 13s 294ms/step - loss: 6.5601 - accuracy: 0.9646 - val_loss: 5.6661 - val_accuracy: 0.9676\n",
        "\n",
        "\n",
        "2020-06-03 17:31:58,629 INFO Training completed\n",
        "\n",
        "2020-06-03 17:31:58,632 INFO accuracy score: 0.96763\n",
        "\n",
        "2020-06-03 17:31:58,633 INFO precision score: 0.93484\n",
        "\n",
        "2020-06-03 17:31:58,635 INFO recall score: 0.93676\n",
        "\n",
        "2020-06-03 17:31:58,636 INFO macro f1 score: 0.93578\n",
        "\n",
        "2020-06-03 17:31:58,638 INFO confusion matrix\n",
        "\n",
        "[[123785    226      0      0      0      0      0      0      0      0  0]\n",
        "\n",
        " [   315  98565    323      0      0      0      0      0      0      0  0]\n",
        "\n",
        " [     0    480  54906    561      0      0      1      0      0      0  0]\n",
        "\n",
        " [     0      3    864  65348    821      0      0      0      0      0  0]\n",
        "\n",
        " [     0      0      7    488  38939    799      0      0      0      0  0]\n",
        "\n",
        " [     0      0      1     28    622  25172    862      0      0      0  0]\n",
        "\n",
        " [     0      0      0      0      2    755  16581   1574      0      0  0]\n",
        "\n",
        " [     0      0      1      0      0      1   1231  23622   1590      0  0]\n",
        "\n",
        " [     0      0      0      0      0      0      0   1636  21700   1238  0]\n",
        "\n",
        " [     0      0      0      2      0      0      0      0    973  12126 426]\n",
        "\n",
        " [     0      0      0      0      0      0      0      0      0    357 3069]]\n",
        "\n",
        "2020-06-03 17:31:58,643 INFO Training completed ...\n",
        "2020-06-03 17:31:58,644 INFO [Running Wavenet model] done in 165 s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sr-bbbpN747",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "eedcf5a6-5537-4a47-b816-5188cda3d5a6"
      },
      "source": [
        "train, feats = prepare_data(fe_config)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-03 16:03:41,794 INFO Reading Data Started ...\n",
            "2020-06-03 16:03:43,981 INFO Reading and Normalizing Data Completed ...\n",
            "2020-06-03 16:03:43,982 INFO [Reading Data] done in 2 s\n",
            "2020-06-03 16:03:43,983 INFO Feature Enginnering Started ...\n",
            "2020-06-03 16:03:46,940 INFO Feature Enginnering Completed ...\n",
            "2020-06-03 16:03:46,941 INFO [Creating Features] done in 3 s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LGS1ldSPSg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(fe_config : List) -> Tuple[pd.DataFrame, List]:\n",
        "    not_feats_cols = ['time']\n",
        "    target_col = ['open_channels']\n",
        "    init_logger()\n",
        "    with timer(f'Reading Data'):\n",
        "        logger.info('Reading Data Started ...')\n",
        "        train, test = read_data()\n",
        "        train, test = normalize(train, test)    \n",
        "        logger.info('Reading and Normalizing Data Completed ...')\n",
        "    with timer(f'Creating Features'):\n",
        "        logger.info('Feature Enginnering Started ...')\n",
        "        for config in fe_config:\n",
        "            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n",
        "        train, feats = feature_selection(train)\n",
        "        logger.info('Feature Enginnering Completed ...')\n",
        "    return train, feats\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQpdtSRPtoAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train and val data split\n",
        "group = train['group']\n",
        "train_id = list()\n",
        "val_id = list()\n",
        "for i in range(0, 5000000, 500000):\n",
        "    train_id.extend(list(range(i, i+450000)))\n",
        "    val_id.extend(list(range(i+450000, i+500000)))\n",
        "train_id = np.asarray(train_id)\n",
        "val_id = np.asarray(val_id)\n",
        "train_val_ids = []\n",
        "train_val_ids.append(np.unique(group[train_id]))\n",
        "train_val_ids.append(np.unique(group[val_id]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS2e2ENpIEmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_cv_model_by_batch(train : pd.DataFrame,\n",
        "                          test : pd.DataFrame,\n",
        "                          splits : int,\n",
        "                          batch_col : Text,\n",
        "                          feats : List,\n",
        "                          nn_epochs : int,\n",
        "                          nn_batch_size : int) -> NoReturn:\n",
        "    \n",
        "    seed_everything(SEED)\n",
        "    K.clear_session()\n",
        "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)\n",
        "    oof_ = np.zeros((len(train), 11))\n",
        "    preds_ = np.zeros((len(test), 11))\n",
        "    target = ['open_channels']\n",
        "    group = train['group']\n",
        "    kf = GroupKFold(n_splits=5)\n",
        "    splits = [x for x in kf.split(train, train[target], group)]\n",
        "\n",
        "    new_splits = []\n",
        "    for sp in splits:\n",
        "        new_split = []\n",
        "        new_split.append(np.unique(group[sp[0]]))\n",
        "        new_split.append(np.unique(group[sp[1]]))\n",
        "        new_split.append(sp[1])    \n",
        "        new_splits.append(new_split)\n",
        "        \n",
        "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
        "\n",
        "    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
        "    target_cols = ['target_'+str(i) for i in range(11)]\n",
        "    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
        "    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
        "    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
        "\n",
        "    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n",
        "        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n",
        "        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n",
        "        \n",
        "        if n_fold < 2:\n",
        "            train_x, train_y = augment(train_x, train_y)\n",
        "\n",
        "        gc.collect()\n",
        "        shape_ = (None, train_x.shape[2])\n",
        "        # print(\"before model classifier\")\n",
        "        model = Classifier(shape_)\n",
        "        # print(\"after model classifier\")\n",
        "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
        "        cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,leave_overall_progress=False, show_epoch_progress=False,show_overall_progress=True)\n",
        "        print(\"before model fit\")\n",
        "       \n",
        "        model.fit(train_x,train_y,\n",
        "                  epochs=nn_epochs,\n",
        "                  callbacks=[cb_prg, cb_lr_schedule],\n",
        "                  batch_size=nn_batch_size,verbose=0,\n",
        "                  validation_data=(valid_x,valid_y))\n",
        "        print(\"after model fit\")\n",
        "        preds_f = model.predict(valid_x)\n",
        "        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n",
        "        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n",
        "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
        "        oof_[val_orig_idx,:] += preds_f\n",
        "        te_preds = model.predict(test)\n",
        "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
        "        preds_ += te_preds / SPLITS\n",
        "    f1_score_ =f1_score(np.argmax(train_tr, axis=2).reshape(-1),  np.argmax(oof_, axis=1), average = 'macro')\n",
        "    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
        "    # sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n",
        "    # sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\n",
        "    # display(sample_submission.head())\n",
        "    np.save('oof.npy', oof_)\n",
        "    np.save('preds.npy', preds_)\n",
        "\n",
        "    return \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}